{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The main code for the feedforward networks assignment.\n",
        "See README.md for details.\n",
        "\"\"\"\n",
        "from typing import Tuple, Dict\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.regularizers import l2  # Import L2 regularization\n",
        "\n",
        "def create_auto_mpg_deep_and_wide_networks(n_inputs: int, n_outputs: int):\n",
        "    \"\"\"Creates a deep and a wide neural network with similar number of parameters.\"\"\"\n",
        "\n",
        "    deep = models.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(48, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    # Further reduced wide model layer sizes\n",
        "    wide = models.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced size more\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced size more\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    # Compile both models\n",
        "    deep.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    wide.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "    # Debugging: Print parameter counts\n",
        "    deep_params = deep.count_params()\n",
        "    wide_params = wide.count_params()\n",
        "    print(f\"Deep network params: {deep_params}, Wide network params: {wide_params}\")\n",
        "\n",
        "    return deep, wide\n",
        "\n",
        "def create_activity_dropout_and_nodropout_networks(n_inputs: int, n_outputs: int):\n",
        "    \"\"\"Creates two identical networks: one with dropout and one without, keeping parameter count identical.\"\"\"\n",
        "\n",
        "    def build_network(use_dropout):\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=(n_inputs,)),\n",
        "            layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Add L2\n",
        "            layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        ])\n",
        "\n",
        "        # Add Dropout layer if 'use_dropout' is True\n",
        "        if use_dropout:\n",
        "            model.add(layers.Dropout(0.5))  # Add a Dropout layer with a dropout rate of 0.5\n",
        "\n",
        "        model.add(layers.Dense(n_outputs, activation='softmax'))  # Use 'softmax' for multi-class output\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    dropout_model = build_network(use_dropout=True)\n",
        "    no_dropout_model = build_network(use_dropout=False)\n",
        "\n",
        "    # Debugging: Print parameter counts\n",
        "    print(f\"Dropout model params: {dropout_model.count_params()}, No-dropout model params: {no_dropout_model.count_params()}\")\n",
        "\n",
        "    return dropout_model, no_dropout_model\n",
        "\n",
        "\n",
        "def create_income_earlystopping_and_noearlystopping_networks(n_inputs: int, n_outputs: int):\n",
        "    \"\"\"Creates two networks: one with early stopping and one without.\"\"\"\n",
        "\n",
        "    def build_model():\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=(n_inputs,)),\n",
        "            layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Keep the layer size larger for better capacity\n",
        "            layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "            layers.Dense(n_outputs, activation='sigmoid')  # Sigmoid activation for binary classification\n",
        "        ])\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    early_model = build_model()\n",
        "    late_model = build_model()\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Fit parameters\n",
        "    early_fit_kwargs = {\"batch_size\": 32, \"epochs\": 50, \"callbacks\": [early_stop]}\n",
        "    late_fit_kwargs = {\"batch_size\": 32, \"epochs\": 100}  # Increase epochs for late model\n",
        "\n",
        "    return early_model, early_fit_kwargs, late_model, late_fit_kwargs"
      ],
      "metadata": {
        "id": "w56N8ZNOwwpW"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}