{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ghyL550iHBxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcb3c2b-4927-4f9d-b785-c8b8a56e2777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded successfully.\n",
            "Data saved to 'auto-mpg.hdf5'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "\n",
        "# Create the 'data/' directory if it doesn't exist\n",
        "data_dir = 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download data using requests\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Data downloaded successfully.\")\n",
        "\n",
        "    data_content = response.text\n",
        "    df = pd.read_csv(StringIO(data_content), header=None, sep=\"\\s+\", na_values=\"?\", names=[\n",
        "        \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
        "        \"acceleration\", \"model year\", \"origin\", \"carname\"])\n",
        "\n",
        "    df = df.dropna().drop(\"carname\", axis=1)\n",
        "    input_df = df.drop(\"mpg\", axis=1)\n",
        "    output_df = df[[\"mpg\"]]\n",
        "\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values\n",
        "    train_output = output_df[mask].values\n",
        "    test_input = input_df[~mask].values\n",
        "    test_output = output_df[~mask].values\n",
        "\n",
        "    with h5py.File(os.path.join(data_dir, 'auto-mpg.hdf5'), 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input)\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output)\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input)\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output)\n",
        "\n",
        "    print(\"Data saved to 'auto-mpg.hdf5'.\")\n",
        "else:\n",
        "    print(f\"Failed to download data. HTTP Status Code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkS1453ie8Mb",
        "outputId": "a9741878-5139-48b3-e4df-3208b56a5b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded successfully.\n",
            "Data saved to: data/uci-har.hdf5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import h5py\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Specify the URL for the UCI HAR Dataset.zip file\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
        "\n",
        "\n",
        "# Download and extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(urllib.request.urlopen(url).read()), 'r') as zip:\n",
        "    train_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/train/X_train.txt\"))\n",
        "    train_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/train/y_train.txt\")))\n",
        "    test_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/test/X_test.txt\"))\n",
        "    test_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/test/y_test.txt\")))\n",
        "\n",
        "# Create an HDF5 file to store the data\n",
        "hdf5_path = 'data/uci-har.hdf5'\n",
        "with h5py.File(hdf5_path, 'w') as f:\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype=np.dtype(\"f2\"))\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype=np.dtype(\"i1\"))\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype=np.dtype(\"f2\"))\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype=np.dtype(\"i1\"))\n",
        "\n",
        "# Print a message indicating that the data has been downloaded and saved\n",
        "print(\"Data downloaded successfully.\")\n",
        "print(\"Data saved to:\", hdf5_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ZpC_I3g4t0",
        "outputId": "9ded41a9-ff14-4ae2-d5ee-e80acc0ef0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded successfully.\n",
            "Data saved to 'income.hdf5'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
        "    \"hours_per_week\", \"native_country\", \"income\"\n",
        "]\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url, header=None, sep=\", \", na_values=\"?\", engine=\"python\", names=column_names)\n",
        "    df = df.dropna()\n",
        "    df = pd.get_dummies(df)\n",
        "    df = df.drop(\"income_<=50K\", axis=1)\n",
        "    input_df = df.drop(\"income_>50K\", axis=1)\n",
        "    output_df = df[[\"income_>50K\"]]\n",
        "\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values\n",
        "    train_output = output_df[mask].values\n",
        "    test_input = input_df[~mask].values\n",
        "    test_output = output_df[~mask].values\n",
        "\n",
        "    with h5py.File('data/income.hdf5', 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype='f')\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype='i')\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype='f')\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype='i')\n",
        "\n",
        "    print(\"Data downloaded successfully.\")\n",
        "    print(\"Data saved to 'income.hdf5'.\")\n",
        "except HTTPError as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w8j_wJxiii5l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_deep_vs_wide(capsys):\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/auto-mpg.hdf5\")\n",
        "\n",
        "    deep, wide = nn.create_auto_mpg_deep_and_wide_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the deep neural network is indeed deeper\n",
        "    assert len(deep.layers) > len(wide.layers)\n",
        "\n",
        "    # check that the 2 networks have (nearly) the same number of parameters\n",
        "    params1 = deep.count_params()\n",
        "    params2 = wide.count_params()\n",
        "    assert abs(params1 - params2) / (params1 + params2) < 0.05\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(deep, wide)\n",
        "\n",
        "    # check that the 2 networks have the same activation functions\n",
        "    assert set(hidden_activations(deep)) == set(hidden_activations(wide))\n",
        "\n",
        "    # check that output type and loss are appropriate for regression\n",
        "    assert all(\"mean\" in loss_name(model) for model in [deep, wide])\n",
        "    assert loss_name(deep) == loss_name(wide)\n",
        "    assert output_activation(deep) == output_activation(wide) == \\\n",
        "        tensorflow.keras.activations.linear\n",
        "\n",
        "    # train both networks\n",
        "    deep.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "    wide.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "\n",
        "    # check that error level is acceptable\n",
        "    mean_predict = np.full(shape=test_out.shape, fill_value=np.mean(train_out))\n",
        "    [baseline_rmse] = root_mean_squared_error(mean_predict, test_out)\n",
        "    [deep_rmse] = root_mean_squared_error(deep.predict(test_in), test_out)\n",
        "    [wide_rmse] = root_mean_squared_error(wide.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        rmse_format = \"{1:.1f} RMSE for {0} on Auto MPG\".format\n",
        "        print()\n",
        "        print(rmse_format(\"baseline\", baseline_rmse))\n",
        "        print(rmse_format(\"deep\", deep_rmse))\n",
        "        print(rmse_format(\"wide\", wide_rmse))\n",
        "\n",
        "    assert deep_rmse < baseline_rmse\n",
        "    assert wide_rmse < baseline_rmse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_dropout(capsys):\n",
        "\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/uci-har.hdf5\")\n",
        "\n",
        "    # keep only every 10th training example\n",
        "    train_out = train_out[::10, :]\n",
        "    train_in = train_in[::10, :]\n",
        "\n",
        "    drop, no_drop = nn.create_activity_dropout_and_nodropout_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the dropout network has Dropout and the other doesn't\n",
        "    assert any(isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in drop.layers)\n",
        "    assert all(not isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same number of parameters\n",
        "    assert drop.count_params() == no_drop.count_params()\n",
        "\n",
        "    # check that the two networks are identical other than dropout\n",
        "    dropped_dropout = [l for l in drop.layers\n",
        "                       if not isinstance(l, tensorflow.keras.layers.Dropout)]\n",
        "    assert_layers_equal(dropped_dropout, no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(drop, no_drop)\n",
        "\n",
        "    # check that output type and loss are appropriate for multi-class\n",
        "    assert all(\"categorical\" in loss_name(model)\n",
        "               for model in [drop, no_drop])\n",
        "    assert loss_name(drop) == loss_name(no_drop)\n",
        "    assert output_activation(drop) == output_activation(no_drop) == \\\n",
        "        tensorflow.keras.activations.softmax\n",
        "\n",
        "    # train both networks\n",
        "    drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "    no_drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "\n",
        "    # check that accuracy level is acceptable\n",
        "    baseline_prediction = np.zeros_like(test_out)\n",
        "    baseline_prediction[:, np.argmax(np.sum(train_out, axis=0), axis=0)] = 1\n",
        "    baseline_accuracy = multi_class_accuracy(baseline_prediction, test_out)\n",
        "    dropout_accuracy = multi_class_accuracy(drop.predict(test_in), test_out)\n",
        "    no_dropout_accuracy = multi_class_accuracy(\n",
        "        no_drop.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on UCI-HAR\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", baseline_accuracy))\n",
        "        print(accuracy_format(\"dropout\", dropout_accuracy))\n",
        "        print(accuracy_format(\"no dropout\", no_dropout_accuracy))\n",
        "    assert dropout_accuracy >= 0.75\n",
        "    assert no_dropout_accuracy >= 0.75\n",
        "\n",
        "\n",
        "def test_early_stopping(capsys):\n",
        "\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/income.hdf5\")\n",
        "\n",
        "    # keep only every 10th training example\n",
        "    train_out = train_out[::10, :]\n",
        "    train_in = train_in[::10, :]\n",
        "\n",
        "    early, early_fit_kwargs, late, late_fit_kwargs = \\\n",
        "        nn.create_income_earlystopping_and_noearlystopping_networks(\n",
        "            train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the two networks have the same number of parameters\n",
        "    assert early.count_params() == late.count_params()\n",
        "\n",
        "    # check that the two networks have identical layers\n",
        "    assert_layers_equal(early.layers, late.layers)\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(early, late)\n",
        "\n",
        "    # check that output type and loss are appropriate for binary classification\n",
        "    assert all(any(x in loss_name(model) for x in {\"crossentropy\", \"hinge\"})\n",
        "               and \"categorical\" not in loss_name(model)\n",
        "               for model in [early, late])\n",
        "    assert loss_name(early) == loss_name(late)\n",
        "    assert output_activation(early) == output_activation(late) == \\\n",
        "        tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # train both networks\n",
        "    late_fit_kwargs.update(verbose=0, epochs=50)\n",
        "    late_hist = late.fit(train_in, train_out, **late_fit_kwargs)\n",
        "    early_fit_kwargs.update(verbose=0, epochs=50,\n",
        "                            validation_data=(test_in, test_out))\n",
        "    early_hist = early.fit(train_in, train_out, **early_fit_kwargs)\n",
        "\n",
        "    # check that accuracy levels are acceptable\n",
        "    all1_accuracy = np.sum(test_out == 1) / test_out.size\n",
        "    early_accuracy = binary_accuracy(early.predict(test_in), test_out)\n",
        "    late_accuracy = binary_accuracy(late.predict(test_in), test_out)\n",
        "    assert early_accuracy > 0.75\n",
        "    assert late_accuracy > 0.75\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on census income\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", all1_accuracy))\n",
        "        print(accuracy_format(\"early\", early_accuracy))\n",
        "        print(accuracy_format(\"late\", late_accuracy))\n",
        "    assert early_accuracy > all1_accuracy\n",
        "    assert late_accuracy > all1_accuracy\n",
        "\n",
        "    # check that the first network stopped early (fewer epochs)\n",
        "    assert len(early_hist.history[\"loss\"]) < len(late_hist.history[\"loss\"])\n",
        "\n",
        "\n",
        "\n",
        "def load_hdf5(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "    return train_in, train_out, test_in, test_out\n",
        "\n",
        "\n",
        "def assert_layers_equal(layers1: List[tensorflow.keras.layers.Layer],\n",
        "                        layers2: List[tensorflow.keras.layers.Layer]):\n",
        "    def layer_info(layer):\n",
        "        return (layer.__class__,\n",
        "                getattr(layer, \"units\", None),\n",
        "                getattr(layer, \"activation\", None))\n",
        "\n",
        "    assert [layer_info(l) for l in layers1] == [layer_info(l) for l in layers2]\n",
        "\n",
        "\n",
        "def assert_compile_parameters_equal(model1: tensorflow.keras.models.Model,\n",
        "                                    model2: tensorflow.keras.models.Model):\n",
        "    def to_dict(obj):\n",
        "        items = dict(__class__=obj.__class__.__name__, **vars(obj))\n",
        "        to_remove = {key for key, value in items.items() if key.endswith(\"_fn\")}\n",
        "        for key in to_remove:\n",
        "            items.pop(key)\n",
        "\n",
        "    assert to_dict(model1.optimizer) == to_dict(model2.optimizer)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def hidden_activations(model):\n",
        "    return [layer.activation\n",
        "            for layer in model.layers[:-1] if hasattr(layer, \"activation\")]\n",
        "\n",
        "\n",
        "def output_activation(model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean(axis=0) ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}