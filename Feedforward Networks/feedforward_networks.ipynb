{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jonyi6gNuYql",
        "outputId": "9ddf0370-07dd-4428-d0b1-838acf6ed0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load nn.py\n",
        "\"\"\"nn\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Vhinm4pCPDNgnCQeFl9feNblwzgjH2IK\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "The main code for the feedforward networks assignment.\n",
        "See README.md for details.\n",
        "\"\"\"\n",
        "from typing import Tuple, Dict\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "def create_auto_mpg_deep_and_wide_networks(\n",
        "        n_inputs: int, n_outputs: int) -> Tuple[tf.keras.models.Model, tf.keras.models.Model]:\n",
        "    \"\"\"Creates one deep neural network and one wide neural network.\n",
        "    The networks should have the same (or very close to the same) number of\n",
        "    parameters and the same activation functions.\n",
        "\n",
        "    The neural networks will be asked to predict the number of miles per gallon\n",
        "    that different cars get. They will be trained and tested on the Auto MPG\n",
        "    dataset from:\n",
        "    https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
        "\n",
        "    :param n_inputs: The number of inputs to the models.\n",
        "    :param n_outputs: The number of outputs from the models.\n",
        "    :return: A tuple of (deep neural network, wide neural network)\n",
        "    \"\"\"\n",
        "    # Deep Neural Network\n",
        "    deep_model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    # Wide Neural Network\n",
        "    wide_model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    return deep_model, wide_model\n",
        "\n",
        "def create_activity_dropout_and_nodropout_networks(\n",
        "        n_inputs: int, n_outputs: int) -> Tuple[tf.keras.models.Model, tf.keras.models.Model]:\n",
        "    \"\"\"Creates one neural network with dropout applied after each layer, and\n",
        "    one neural network without dropout. The networks should be identical other\n",
        "    than the presence or absence of dropout.\n",
        "\n",
        "    The neural networks will be asked to predict which one of six activity types\n",
        "    a smartphone user was performing. They will be trained and tested on the\n",
        "    UCI-HAR dataset from:\n",
        "    https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
        "\n",
        "    :param n_inputs: The number of inputs to the models.\n",
        "    :param n_outputs: The number of outputs from the models.\n",
        "    :return: A tuple of (dropout neural network, no-dropout neural network)\n",
        "    \"\"\"\n",
        "    # Dropout Neural Network\n",
        "    dropout_model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(n_outputs, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # No-Dropout Neural Network\n",
        "    no_dropout_model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return dropout_model, no_dropout_model\n",
        "\n",
        "def create_income_earlystopping_and_noearlystopping_networks(\n",
        "        n_inputs: int, n_outputs: int) -> Tuple[tf.keras.models.Model, Dict, tf.keras.models.Model, Dict]:\n",
        "    \"\"\"Creates one neural network that uses early stopping during training, and\n",
        "    one that does not. The networks should be identical other than the presence\n",
        "    or absence of early stopping.\n",
        "\n",
        "    The neural networks will be asked to predict whether a person makes more\n",
        "    than $50K per year. They will be trained and tested on the \"adult\" dataset\n",
        "    from:\n",
        "    https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "    :param n_inputs: The number of inputs to the models.\n",
        "    :param n_outputs: The number of outputs from the models.\n",
        "    :return: A tuple of (\n",
        "        early-stopping neural network,\n",
        "        early-stopping parameters that should be passed to Model.fit,\n",
        "        no-early-stopping neural network,\n",
        "        no-early-stopping parameters that should be passed to Model.fit\n",
        "    )\n",
        "    \"\"\"\n",
        "    # Early Stopping Neural Network\n",
        "    early_stopping_model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    early_stopping_params = {\n",
        "        'callbacks': [callbacks.EarlyStopping(monitor='val_loss', patience=3)],\n",
        "        'validation_split': 0.2\n",
        "    }\n",
        "\n",
        "    # No Early Stopping Neural Network\n",
        "    no_early_stopping_model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(n_inputs,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    no_early_stopping_params = {\n",
        "        'validation_split': 0.2\n",
        "    }\n",
        "\n",
        "    return early_stopping_model, early_stopping_params, no_early_stopping_model, no_early_stopping_params"
      ],
      "metadata": {
        "id": "FPaHkAUfudlk"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nn.py', 'r') as file:\n",
        "    code = file.read()"
      ],
      "metadata": {
        "id": "C3p5eeOYuhm-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5uLwEkwujq3",
        "outputId": "8b5b01bd-df5e-45d1-c3c3-d34e5d7a7788"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1Vhinm4pCPDNgnCQeFl9feNblwzgjH2IK\n",
            "\"\"\"\n",
            "\n",
            "\"\"\"\n",
            "The main code for the feedforward networks assignment.\n",
            "See README.md for details.\n",
            "\"\"\"\n",
            "from typing import Tuple, Dict\n",
            "import tensorflow as tf\n",
            "from tensorflow import keras\n",
            "from tensorflow.keras import layers, models\n",
            "from tensorflow.keras import optimizers\n",
            "from tensorflow.keras import callbacks\n",
            "from tensorflow.keras.regularizers import l2  # Import L2 regularization\n",
            "\n",
            "def create_auto_mpg_deep_and_wide_networks(n_inputs: int, n_outputs: int):\n",
            "    \"\"\"Creates a deep and a wide neural network with similar number of parameters.\"\"\"\n",
            "\n",
            "    deep = models.Sequential([\n",
            "        layers.Input(shape=(n_inputs,)),\n",
            "        layers.Dense(64, activation='relu'),\n",
            "        layers.Dense(48, activation='relu'),\n",
            "        layers.Dense(32, activation='relu'),\n",
            "        layers.Dense(n_outputs)\n",
            "    ])\n",
            "\n",
            "    # Further reduced wide model layer sizes\n",
            "    wide = models.Sequential([\n",
            "        layers.Input(shape=(n_inputs,)),\n",
            "        layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced size more\n",
            "        layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced size more\n",
            "        layers.Dense(n_outputs)\n",
            "    ])\n",
            "\n",
            "    # Compile both models\n",
            "    deep.compile(optimizer=\"adam\", loss=\"mse\")\n",
            "    wide.compile(optimizer=\"adam\", loss=\"mse\")\n",
            "\n",
            "    # Debugging: Print parameter counts\n",
            "    deep_params = deep.count_params()\n",
            "    wide_params = wide.count_params()\n",
            "    print(f\"Deep network params: {deep_params}, Wide network params: {wide_params}\")\n",
            "\n",
            "    return deep, wide\n",
            "\n",
            "def create_activity_dropout_and_nodropout_networks(n_inputs: int, n_outputs: int):\n",
            "    \"\"\"Creates two identical networks: one with dropout and one without, keeping parameter count identical.\"\"\"\n",
            "\n",
            "    def build_network(use_dropout):\n",
            "        model = models.Sequential([\n",
            "            layers.Input(shape=(n_inputs,)),\n",
            "            layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Add L2\n",
            "            layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
            "        ])\n",
            "\n",
            "        # Add Dropout layer if 'use_dropout' is True\n",
            "        if use_dropout:\n",
            "            model.add(layers.Dropout(0.5))  # Add a Dropout layer with a dropout rate of 0.5\n",
            "\n",
            "        model.add(layers.Dense(n_outputs, activation='softmax'))  # Use 'softmax' for multi-class output\n",
            "        model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
            "        return model\n",
            "\n",
            "    dropout_model = build_network(use_dropout=True)\n",
            "    no_dropout_model = build_network(use_dropout=False)\n",
            "\n",
            "    # Debugging: Print parameter counts\n",
            "    print(f\"Dropout model params: {dropout_model.count_params()}, No-dropout model params: {no_dropout_model.count_params()}\")\n",
            "\n",
            "    return dropout_model, no_dropout_model\n",
            "\n",
            "\n",
            "def create_income_earlystopping_and_noearlystopping_networks(n_inputs: int, n_outputs: int):\n",
            "    \"\"\"Creates two networks: one with early stopping and one without.\"\"\"\n",
            "\n",
            "    def build_model():\n",
            "        model = models.Sequential([\n",
            "            layers.Input(shape=(n_inputs,)),\n",
            "            layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Keep the layer size larger for better capacity\n",
            "            layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
            "            layers.Dense(n_outputs, activation='sigmoid')  # Sigmoid activation for binary classification\n",
            "        ])\n",
            "        model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
            "        return model\n",
            "\n",
            "    early_model = build_model()\n",
            "    late_model = build_model()\n",
            "\n",
            "    # Early stopping callback\n",
            "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
            "\n",
            "    # Fit parameters\n",
            "    early_fit_kwargs = {\"batch_size\": 32, \"epochs\": 50, \"callbacks\": [early_stop]}\n",
            "    late_fit_kwargs = {\"batch_size\": 32, \"epochs\": 100}  # Increase epochs for late model\n",
            "\n",
            "    return early_model, early_fit_kwargs, late_model, late_fit_kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load test_nn.py\n",
        "\"\"\"test_nn\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1nVEyMWWFM0iToa5w6bjJc7B0ujKfr3ho\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "\n",
        "# Create the 'data/' directory if it doesn't exist\n",
        "data_dir = 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download data using requests\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Data downloaded successfully.\")\n",
        "\n",
        "    data_content = response.text\n",
        "    df = pd.read_csv(StringIO(data_content), header=None, sep=\"\\s+\", na_values=\"?\", names=[\n",
        "        \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
        "        \"acceleration\", \"model year\", \"origin\", \"carname\"])\n",
        "\n",
        "    df = df.dropna().drop(\"carname\", axis=1)\n",
        "    input_df = df.drop(\"mpg\", axis=1)\n",
        "    output_df = df[[\"mpg\"]]\n",
        "\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values\n",
        "    train_output = output_df[mask].values\n",
        "    test_input = input_df[~mask].values\n",
        "    test_output = output_df[~mask].values\n",
        "\n",
        "    with h5py.File(os.path.join(data_dir, 'auto-mpg.hdf5'), 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input)\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output)\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input)\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output)\n",
        "\n",
        "    print(\"Data saved to 'auto-mpg.hdf5'.\")\n",
        "else:\n",
        "    print(f\"Failed to download data. HTTP Status Code: {response.status_code}\")\n",
        "\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import h5py\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Specify the URL for the UCI HAR Dataset.zip file\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
        "\n",
        "\n",
        "# Download and extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(urllib.request.urlopen(url).read()), 'r') as zip:\n",
        "    train_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/train/X_train.txt\"))\n",
        "    train_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/train/y_train.txt\")))\n",
        "    test_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/test/X_test.txt\"))\n",
        "    test_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/test/y_test.txt\")))\n",
        "\n",
        "# Create an HDF5 file to store the data\n",
        "hdf5_path = 'data/uci-har.hdf5'\n",
        "with h5py.File(hdf5_path, 'w') as f:\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype=np.dtype(\"f2\"))\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype=np.dtype(\"i1\"))\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype=np.dtype(\"f2\"))\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype=np.dtype(\"i1\"))\n",
        "\n",
        "# Print a message indicating that the data has been downloaded and saved\n",
        "print(\"Data downloaded successfully.\")\n",
        "print(\"Data saved to:\", hdf5_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "from urllib.error import HTTPError\n",
        "import os\n",
        "\n",
        "# Create the 'data' directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
        "    \"hours_per_week\", \"native_country\", \"income\"\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(url, header=None, sep=\", \", na_values=\"?\", engine=\"python\", names=column_names)\n",
        "    df = df.dropna()  # Drop rows with missing values\n",
        "    df = pd.get_dummies(df)  # Convert categorical variables to one-hot encoding\n",
        "    df = df.drop(\"income_<=50K\", axis=1)  # Drop one of the target columns to avoid redundancy\n",
        "    input_df = df.drop(\"income_>50K\", axis=1)  # Input features\n",
        "    output_df = df[[\"income_>50K\"]]  # Target column\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values.astype('float32')  # Convert to float32\n",
        "    train_output = output_df[mask].values.astype('int32')  # Convert to int32\n",
        "    test_input = input_df[~mask].values.astype('float32')  # Convert to float32\n",
        "    test_output = output_df[~mask].values.astype('int32')  # Convert to int32\n",
        "\n",
        "    # Save the data to an HDF5 file\n",
        "    with h5py.File('data/income.hdf5', 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype='float32')\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype='int32')\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype='float32')\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype='int32')\n",
        "\n",
        "    print(\"Data downloaded successfully.\")\n",
        "    print(\"Data saved to 'data/income.hdf5'.\")\n",
        "except HTTPError as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_deep_vs_wide(capsys):\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/auto-mpg.hdf5\")\n",
        "\n",
        "    deep, wide = nn.create_auto_mpg_deep_and_wide_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the deep neural network is indeed deeper\n",
        "    assert len(deep.layers) > len(wide.layers)\n",
        "\n",
        "    # check that the 2 networks have (nearly) the same number of parameters\n",
        "    params1 = deep.count_params()\n",
        "    params2 = wide.count_params()\n",
        "    assert abs(params1 - params2) / (params1 + params2) < 0.05\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(deep, wide)\n",
        "\n",
        "    # check that the 2 networks have the same activation functions\n",
        "    assert set(hidden_activations(deep)) == set(hidden_activations(wide))\n",
        "\n",
        "    # check that output type and loss are appropriate for regression\n",
        "    assert all(\"mean\" in loss_name(model) for model in [deep, wide])\n",
        "    assert loss_name(deep) == loss_name(wide)\n",
        "    assert output_activation(deep) == output_activation(wide) == \\\n",
        "        tensorflow.keras.activations.linear\n",
        "\n",
        "    # train both networks\n",
        "    deep.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "    wide.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "\n",
        "    # check that error level is acceptable\n",
        "    mean_predict = np.full(shape=test_out.shape, fill_value=np.mean(train_out))\n",
        "    [baseline_rmse] = root_mean_squared_error(mean_predict, test_out)\n",
        "    [deep_rmse] = root_mean_squared_error(deep.predict(test_in), test_out)\n",
        "    [wide_rmse] = root_mean_squared_error(wide.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        rmse_format = \"{1:.1f} RMSE for {0} on Auto MPG\".format\n",
        "        print()\n",
        "        print(rmse_format(\"baseline\", baseline_rmse))\n",
        "        print(rmse_format(\"deep\", deep_rmse))\n",
        "        print(rmse_format(\"wide\", wide_rmse))\n",
        "\n",
        "    assert deep_rmse < baseline_rmse\n",
        "    assert wide_rmse < baseline_rmse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_dropout(capsys):\n",
        "\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/uci-har.hdf5\")\n",
        "\n",
        "    # keep only every 10th training example\n",
        "    train_out = train_out[::10, :]\n",
        "    train_in = train_in[::10, :]\n",
        "\n",
        "    drop, no_drop = nn.create_activity_dropout_and_nodropout_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the dropout network has Dropout and the other doesn't\n",
        "    assert any(isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in drop.layers)\n",
        "    assert all(not isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same number of parameters\n",
        "    assert drop.count_params() == no_drop.count_params()\n",
        "\n",
        "    # check that the two networks are identical other than dropout\n",
        "    dropped_dropout = [l for l in drop.layers\n",
        "                       if not isinstance(l, tensorflow.keras.layers.Dropout)]\n",
        "    assert_layers_equal(dropped_dropout, no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(drop, no_drop)\n",
        "\n",
        "    # check that output type and loss are appropriate for multi-class\n",
        "    assert all(\"categorical\" in loss_name(model)\n",
        "               for model in [drop, no_drop])\n",
        "    assert loss_name(drop) == loss_name(no_drop)\n",
        "    assert output_activation(drop) == output_activation(no_drop) == \\\n",
        "        tensorflow.keras.activations.softmax\n",
        "\n",
        "    # train both networks\n",
        "    drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "    no_drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "\n",
        "    # check that accuracy level is acceptable\n",
        "    baseline_prediction = np.zeros_like(test_out)\n",
        "    baseline_prediction[:, np.argmax(np.sum(train_out, axis=0), axis=0)] = 1\n",
        "    baseline_accuracy = multi_class_accuracy(baseline_prediction, test_out)\n",
        "    dropout_accuracy = multi_class_accuracy(drop.predict(test_in), test_out)\n",
        "    no_dropout_accuracy = multi_class_accuracy(\n",
        "        no_drop.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on UCI-HAR\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", baseline_accuracy))\n",
        "        print(accuracy_format(\"dropout\", dropout_accuracy))\n",
        "        print(accuracy_format(\"no dropout\", no_dropout_accuracy))\n",
        "    assert dropout_accuracy >= 0.75\n",
        "    assert no_dropout_accuracy >= 0.75\n",
        "\n",
        "\n",
        "def test_early_stopping(capsys):\n",
        "\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/income.hdf5\")\n",
        "\n",
        "    # keep only every 10th training example\n",
        "    train_out = train_out[::10, :]\n",
        "    train_in = train_in[::10, :]\n",
        "\n",
        "    early, early_fit_kwargs, late, late_fit_kwargs = \\\n",
        "        nn.create_income_earlystopping_and_noearlystopping_networks(\n",
        "            train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the two networks have the same number of parameters\n",
        "    assert early.count_params() == late.count_params()\n",
        "\n",
        "    # check that the two networks have identical layers\n",
        "    assert_layers_equal(early.layers, late.layers)\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(early, late)\n",
        "\n",
        "    # check that output type and loss are appropriate for binary classification\n",
        "    assert all(any(x in loss_name(model) for x in {\"crossentropy\", \"hinge\"})\n",
        "               and \"categorical\" not in loss_name(model)\n",
        "               for model in [early, late])\n",
        "    assert loss_name(early) == loss_name(late)\n",
        "    assert output_activation(early) == output_activation(late) == \\\n",
        "        tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # train both networks\n",
        "    late_fit_kwargs.update(verbose=0, epochs=50)\n",
        "    late_hist = late.fit(train_in, train_out, **late_fit_kwargs)\n",
        "    early_fit_kwargs.update(verbose=0, epochs=50,\n",
        "                            validation_data=(test_in, test_out))\n",
        "    early_hist = early.fit(train_in, train_out, **early_fit_kwargs)\n",
        "\n",
        "    # check that accuracy levels are acceptable\n",
        "    all1_accuracy = np.sum(test_out == 1) / test_out.size\n",
        "    early_accuracy = binary_accuracy(early.predict(test_in), test_out)\n",
        "    late_accuracy = binary_accuracy(late.predict(test_in), test_out)\n",
        "    assert early_accuracy > 0.75\n",
        "    assert late_accuracy > 0.75\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on census income\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", all1_accuracy))\n",
        "        print(accuracy_format(\"early\", early_accuracy))\n",
        "        print(accuracy_format(\"late\", late_accuracy))\n",
        "    assert early_accuracy > all1_accuracy\n",
        "    assert late_accuracy > all1_accuracy\n",
        "\n",
        "    # check that the first network stopped early (fewer epochs)\n",
        "    assert len(early_hist.history[\"loss\"]) < len(late_hist.history[\"loss\"])\n",
        "\n",
        "\n",
        "\n",
        "def load_hdf5(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "    return train_in, train_out, test_in, test_out\n",
        "\n",
        "\n",
        "def assert_layers_equal(layers1: List[tensorflow.keras.layers.Layer],\n",
        "                        layers2: List[tensorflow.keras.layers.Layer]):\n",
        "    def layer_info(layer):\n",
        "        return (layer.__class__,\n",
        "                getattr(layer, \"units\", None),\n",
        "                getattr(layer, \"activation\", None))\n",
        "\n",
        "    assert [layer_info(l) for l in layers1] == [layer_info(l) for l in layers2]\n",
        "\n",
        "\n",
        "def assert_compile_parameters_equal(model1: tensorflow.keras.models.Model,\n",
        "                                    model2: tensorflow.keras.models.Model):\n",
        "    def to_dict(obj):\n",
        "        items = dict(__class__=obj.__class__.__name__, **vars(obj))\n",
        "        to_remove = {key for key, value in items.items() if key.endswith(\"_fn\")}\n",
        "        for key in to_remove:\n",
        "            items.pop(key)\n",
        "\n",
        "    assert to_dict(model1.optimizer) == to_dict(model2.optimizer)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def hidden_activations(model):\n",
        "    return [layer.activation\n",
        "            for layer in model.layers[:-1] if hasattr(layer, \"activation\")]\n",
        "\n",
        "\n",
        "def output_activation(model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean(axis=0) ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)"
      ],
      "metadata": {
        "id": "P_eYm3MAumVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383dd13f-deaa-467f-c8c3-8a7b4746fe2e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded successfully.\n",
            "Data saved to 'auto-mpg.hdf5'.\n",
            "Data downloaded successfully.\n",
            "Data saved to: data/uci-har.hdf5\n",
            "Data downloaded successfully.\n",
            "Data saved to 'data/income.hdf5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xxaXL9PuojZ",
        "outputId": "f8364dd5-fd21-49d5-a01e-755bb7e88432"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.3.8, typeguard-4.4.1, anyio-3.7.1\n",
            "collected 3 items                                                                                  \u001b[0m\n",
            "\n",
            "test_nn.py \n",
            "7.5 RMSE for baseline on Auto MPG\n",
            "6.1 RMSE for deep on Auto MPG\n",
            "6.4 RMSE for wide on Auto MPG\n",
            "\u001b[32m.\u001b[0m\n",
            "18.2% accuracy for baseline on UCI-HAR\n",
            "86.3% accuracy for dropout on UCI-HAR\n",
            "84.4% accuracy for no dropout on UCI-HAR\n",
            "\u001b[32m.\u001b[0m\n",
            "24.9% accuracy for baseline on census income\n",
            "78.8% accuracy for early on census income\n",
            "79.2% accuracy for late on census income\n",
            "\u001b[32m.\u001b[0m\u001b[32m                                                                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 56.16s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}