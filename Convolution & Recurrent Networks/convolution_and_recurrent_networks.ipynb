{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l9meHej4v3H",
        "outputId": "e16db91c-3826-4cc6-9a05-c2778a3cf8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load nn.py\n",
        "\"\"\"nn\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1mED2_1ArZUU_cwO9x5rhRG1VRQldrL4p\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "def create_toy_rnn(input_shape: tuple, n_outputs: int) -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
        "    # Define the input layer\n",
        "    inputs = tensorflow.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Add an LSTM layer (a type of RNN)\n",
        "    x = tensorflow.keras.layers.LSTM(64, return_sequences=True)(inputs)\n",
        "\n",
        "    # Add a Dense layer to produce the output\n",
        "    outputs = tensorflow.keras.layers.Dense(n_outputs, activation=\"linear\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tensorflow.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "    # Return the model and any additional arguments for Model.fit\n",
        "    return model, {\"batch_size\": 32, \"epochs\": 20}\n",
        "def create_mnist_cnn(input_shape: tuple, n_outputs: int) -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
        "    # Define the input layer\n",
        "    inputs = tensorflow.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Add convolutional layers\n",
        "    x = tensorflow.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
        "    x = tensorflow.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tensorflow.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "    x = tensorflow.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Flatten the output for the dense layers\n",
        "    x = tensorflow.keras.layers.Flatten()(x)\n",
        "\n",
        "    # Add dense layers\n",
        "    x = tensorflow.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    outputs = tensorflow.keras.layers.Dense(n_outputs, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tensorflow.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Return the model and any additional arguments for Model.fit\n",
        "    return model, {\"batch_size\": 32, \"epochs\": 10}\n",
        "\n",
        "def create_youtube_comment_rnn(vocabulary: List[str], n_outputs: int) -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
        "    # Define the input layer\n",
        "    inputs = tensorflow.keras.Input(shape=(None,))  # Variable-length sequences\n",
        "\n",
        "    # Add an embedding layer to convert token indices to dense vectors\n",
        "    x = tensorflow.keras.layers.Embedding(input_dim=len(vocabulary), output_dim=64)(inputs)\n",
        "\n",
        "    # Add an LSTM layer\n",
        "    x = tensorflow.keras.layers.LSTM(64)(x)\n",
        "\n",
        "    # Add a Dense layer to produce the output\n",
        "    outputs = tensorflow.keras.layers.Dense(n_outputs, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tensorflow.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Return the model and any additional arguments for Model.fit\n",
        "    return model, {\"batch_size\": 32, \"epochs\": 10}\n",
        "\n",
        "def create_youtube_comment_cnn(vocabulary: List[str], n_outputs: int) -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
        "    # Define the input layer\n",
        "    inputs = tensorflow.keras.Input(shape=(None,))  # Variable-length sequences\n",
        "\n",
        "    # Add an embedding layer to convert token indices to dense vectors\n",
        "    x = tensorflow.keras.layers.Embedding(input_dim=len(vocabulary), output_dim=64)(inputs)\n",
        "\n",
        "    # Add 1D convolutional layers\n",
        "    x = tensorflow.keras.layers.Conv1D(64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = tensorflow.keras.layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Add a Dense layer to produce the output\n",
        "    outputs = tensorflow.keras.layers.Dense(n_outputs, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tensorflow.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Return the model and any additional arguments for Model.fit\n",
        "    return model, {\"batch_size\": 32, \"epochs\": 10}"
      ],
      "metadata": {
        "id": "2IaQ6zBu5Dm1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nn.py', 'r') as file:\n",
        "    code = file.read()"
      ],
      "metadata": {
        "id": "Q7L4CG-35FRQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgWlrnZD5HFF",
        "outputId": "e1d7dfdb-aeeb-4a62-9010-e8284fe2e479"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1mED2_1ArZUU_cwO9x5rhRG1VRQldrL4p\n",
            "\"\"\"\n",
            "\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1mED2_1ArZUU_cwO9x5rhRG1VRQldrL4p\n",
            "\"\"\"\n",
            "\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential, Model\n",
            "from tensorflow.keras.layers import (\n",
            "    Embedding, LSTM, Dense, Dropout, Bidirectional, Conv2D, MaxPooling2D, Flatten,\n",
            "    BatchNormalization, GlobalMaxPooling1D, Conv1D, Input, Normalization\n",
            ")\n",
            "from tensorflow.keras.optimizers import Adam\n",
            "from tensorflow.keras.losses import MeanSquaredError\n",
            "from tensorflow.keras.layers import TimeDistributed, Dense\n",
            "from tensorflow.keras.layers import LayerNormalization\n",
            "from tensorflow.keras.losses import Huber\n",
            "from tensorflow.keras.regularizers import l2\n",
            "from tensorflow.keras.layers import SpatialDropout1D\n",
            "from tensorflow.keras.layers import GRU\n",
            "import tensorflow.keras as keras\n",
            "\n",
            "def create_toy_rnn(input_shape, n_outputs):\n",
            "    print(\"Shape of input data:\", input_shape)\n",
            "\n",
            "    model = keras.Sequential([\n",
            "        keras.layers.Input(shape=input_shape),  #Explicit Input layer\n",
            "        keras.layers.SimpleRNN(128, activation=\"tanh\", return_sequences=True,\n",
            "                               kernel_initializer=\"glorot_uniform\",\n",
            "                               kernel_regularizer=keras.regularizers.l2(0.001)),  # L2 regularization\n",
            "        keras.layers.Dropout(0.1),  #Lower dropout to retain more info\n",
            "        keras.layers.SimpleRNN(64, activation=\"tanh\", return_sequences=True),\n",
            "        keras.layers.Dense(32, activation=\"relu\"),  #Extra dense layer for better feature extraction\n",
            "        keras.layers.Dense(n_outputs, activation=\"linear\")\n",
            "    ])\n",
            "    optimizer = keras.optimizers.RMSprop(learning_rate=0.002)  #RMSprop is often better for RNNs\n",
            "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
            "    return model, {}\n",
            "\n",
            "def create_mnist_cnn(input_shape: tuple, n_outputs: int):\n",
            "    inputs = Input(shape=input_shape)\n",
            "    x = Conv2D(32, kernel_size=(3, 3), padding=\"same\")(inputs)\n",
            "    x = BatchNormalization()(x)\n",
            "    x = tf.keras.activations.relu(x)\n",
            "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
            "    x = Conv2D(64, kernel_size=(3, 3), padding=\"same\")(x)\n",
            "    x = BatchNormalization()(x)\n",
            "    x = tf.keras.activations.relu(x)\n",
            "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
            "    x = Conv2D(128, kernel_size=(3, 3), padding=\"same\")(x)\n",
            "    x = BatchNormalization()(x)\n",
            "    x = tf.keras.activations.relu(x)\n",
            "    x = Flatten()(x)\n",
            "    x = Dense(128, activation=\"relu\")(x)\n",
            "    x = Dropout(0.3)(x)  #Reduced dropout for better learning\n",
            "    outputs = Dense(n_outputs, activation=\"softmax\")(x)\n",
            "    model = Model(inputs, outputs)\n",
            "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
            "                  loss=\"categorical_crossentropy\",\n",
            "                  metrics=[\"accuracy\"])\n",
            "    return model, {\"epochs\": 12, \"batch_size\": 32}\n",
            "\n",
            "def create_youtube_comment_rnn(vocabulary, n_outputs):\n",
            "    vocab_size = len(vocabulary) + 1\n",
            "    embedding_dim = 128\n",
            "    model = Sequential([\n",
            "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
            "        SpatialDropout1D(0.3),\n",
            "        Bidirectional(LSTM(128, return_sequences=True)),\n",
            "        GRU(64, return_sequences=False),\n",
            "        Dropout(0.3),\n",
            "        Dense(32, activation='relu'),\n",
            "        Dense(n_outputs, activation='sigmoid' if n_outputs == 1 else 'softmax')\n",
            "    ])\n",
            "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
            "                  loss=\"binary_crossentropy\" if n_outputs == 1 else \"categorical_crossentropy\",\n",
            "                  metrics=[\"accuracy\"])\n",
            "\n",
            "    return model, {\"epochs\": 12, \"batch_size\": 32}\n",
            "\n",
            "def create_youtube_comment_cnn(vocabulary, n_outputs):\n",
            "    inputs = Input(shape=(None,))\n",
            "    x = Embedding(input_dim=len(vocabulary) + 1, output_dim=64)(inputs)\n",
            "    x = Conv1D(64, kernel_size=3, activation=\"relu\")(x)\n",
            "    x = GlobalMaxPooling1D()(x)\n",
            "    outputs = Dense(n_outputs, activation=\"sigmoid\" if n_outputs == 1 else \"softmax\")(x)\n",
            "    model = Model(inputs, outputs)\n",
            "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\" if n_outputs == 1 else \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
            "    return model, {\"epochs\": 10, \"batch_size\": 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load test_nn.py\n",
        "\"\"\"test_nn\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cm7gAHlV3gMp1L1_x-PfQ3G014opGw4j\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_input, train_output), (test_input, test_output) = mnist.load_data()\n",
        "\n",
        "# Expand dimensions to make compatible with CNN input shape\n",
        "train_input = np.expand_dims(train_input, axis=-1)\n",
        "test_input = np.expand_dims(test_input, axis=-1)\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "train_output = to_categorical(train_output).astype(np.float32)\n",
        "test_output = to_categorical(test_output).astype(np.float32)\n",
        "\n",
        "# Create 'data' directory if it doesn't exist\n",
        "if not os.path.exists('data'):  # For Assignment data is provided in hdf5 format\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Save preprocessed data to HDF5 file\n",
        "with h5py.File('data/mnist.hdf5', 'w') as f:\n",
        "    # Include only every 100th training/testing example to limit dataset size\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input[::100])\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output[::100])\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input[::100])\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output[::100])\n",
        "\n",
        "import json\n",
        "import re\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download .csv files from\n",
        "    #     https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
        "names = [\"1-Psy\", \"2-KatyPerry\", \"3-LMFAO\",\"4-Eminem\", \"5-Shakira\"]  # For Assignment all 5 datasets are provided\n",
        "dfs = [pd.read_csv(\"data/Youtube0{0}.csv\".format(name)) for name in names]\n",
        "tokenize = re.compile(r\"\\d+|[^\\d\\W]+|\\S\").findall\n",
        "dfs_tokenized = [[tokenize(comment) for comment in df[\"CONTENT\"]]\n",
        "                 for df in dfs]\n",
        "\n",
        "index_to_token = [''] + sorted(set(token\n",
        "                                   for comments in dfs_tokenized\n",
        "                                   for tokens in comments\n",
        "                                   for token in tokens))\n",
        "\n",
        "token_to_index = {c: i for i, c in enumerate(index_to_token)}\n",
        "\n",
        "max_tokens = max(len(tokens)\n",
        "                 for comments in dfs_tokenized\n",
        "                 for tokens in comments)\n",
        "\n",
        "with h5py.File('data/youtube-comments.hdf5', 'w') as f:\n",
        "    f.attrs[\"vocabulary\"] = json.dumps(index_to_token)\n",
        "    for name, df, comments in zip(names, dfs, dfs_tokenized):\n",
        "        matrix_in = np.zeros(shape=(len(comments), max_tokens))\n",
        "        for i, tokens in enumerate(comments):\n",
        "            for j, token in enumerate(tokens):\n",
        "                matrix_in[i, j] = token_to_index[token]\n",
        "        matrix_out = df[\"CLASS\"].values.reshape((-1, 1))\n",
        "        group = f.create_group(name)\n",
        "        group.create_dataset(\"input\", compression=\"gzip\", data=matrix_in)\n",
        "        group.create_dataset(\"output\", compression=\"gzip\", data=matrix_out)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_toy_rnn(capsys):\n",
        "    n_train = 20\n",
        "    n_test = 10\n",
        "    n_timesteps = 20\n",
        "    n_features = 2\n",
        "\n",
        "    # create random input for train and test\n",
        "    train_in = np.random.randint(1, 11, (n_train, n_timesteps, n_features))\n",
        "    test_in = np.random.randint(1, 11, (n_test, n_timesteps, n_features))\n",
        "\n",
        "    # deterministically create output from the random input\n",
        "    def out(matrix_in):\n",
        "        matrix_out = np.zeros(shape=matrix_in.shape[:-1] + (1,))\n",
        "        for i, example in enumerate(matrix_in):\n",
        "            for j, [_, x1] in enumerate(example):\n",
        "                [x0, _] = example[j - 3] if j >= 3 else [0., 0.]\n",
        "                matrix_out[i, j] = x0 - x1\n",
        "        return matrix_out\n",
        "    train_out = out(train_in)\n",
        "    test_out = out(test_in)\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, _, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_toy_rnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"mean\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.linear\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=20, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure error is low enough\n",
        "    rmse = root_mean_squared_error(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1f} RMSE for RNN on toy problem\".format(rmse))\n",
        "    assert rmse < 2\n",
        "\n",
        "\n",
        "def test_image_cnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/mnist.hdf5\", 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_mnist_cnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"categorical\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.softmax\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = multi_class_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on MNIST sample\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_rnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_rnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for RNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_cnn(capsys):\n",
        "    # The data below was obtained as in test_text_rnn\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_cnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def layers(model: tensorflow.keras.models.Model):\n",
        "    return [x.layer if isinstance(x, tensorflow.keras.layers.Wrapper) else x\n",
        "            for x in model.layers]\n",
        "\n",
        "\n",
        "def is_convolution(layer: tensorflow.keras.layers.Layer):\n",
        "    return layer.__class__.__name__.startswith('Conv')\n",
        "\n",
        "\n",
        "def is_recurrent(layer: tensorflow.keras.layers.Layer):\n",
        "    return isinstance(layer, tensorflow.keras.layers.RNN)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def output_activation(model: tensorflow.keras.models.Model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean() ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)"
      ],
      "metadata": {
        "id": "zRjtdA3d5Mum"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFoMQ48D5PCZ",
        "outputId": "f7db1691-1919-4ebc-c143-ea46deae8d2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.2, langsmith-0.3.8\n",
            "collected 4 items                                                                                  \u001b[0m\n",
            "\n",
            "test_nn.py \n",
            "1.5 RMSE for RNN on toy problem\n",
            "\u001b[32m.\u001b[0m\n",
            "93.0% accuracy for CNN on MNIST sample\n",
            "\u001b[32m.\u001b[0m\n",
            "87.3% accuracy for RNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\n",
            "84.9% accuracy for CNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\u001b[32m                                                                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 129.26s (0:02:09)\u001b[0m\u001b[32m ===================================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}