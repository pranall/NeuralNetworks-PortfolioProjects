{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"nn\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1mED2_1ArZUU_cwO9x5rhRG1VRQldrL4p\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, LSTM, Dense, Dropout, Bidirectional, Conv2D, MaxPooling2D, Flatten,\n",
        "    BatchNormalization, GlobalMaxPooling1D, Conv1D, Input, Normalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.layers import TimeDistributed, Dense\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.layers import GRU\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "def create_toy_rnn(input_shape, n_outputs):\n",
        "    print(\"Shape of input data:\", input_shape)\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),  #Explicit Input layer\n",
        "        keras.layers.SimpleRNN(128, activation=\"tanh\", return_sequences=True,\n",
        "                               kernel_initializer=\"glorot_uniform\",\n",
        "                               kernel_regularizer=keras.regularizers.l2(0.001)),  # L2 regularization\n",
        "        keras.layers.Dropout(0.1),  #Lower dropout to retain more info\n",
        "        keras.layers.SimpleRNN(64, activation=\"tanh\", return_sequences=True),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),  #Extra dense layer for better feature extraction\n",
        "        keras.layers.Dense(n_outputs, activation=\"linear\")\n",
        "    ])\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=0.002)  #RMSprop is often better for RNNs\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model, {}\n",
        "\n",
        "def create_mnist_cnn(input_shape: tuple, n_outputs: int):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, kernel_size=(3, 3), padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)  #Reduced dropout for better learning\n",
        "    outputs = Dense(n_outputs, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model, {\"epochs\": 12, \"batch_size\": 32}\n",
        "\n",
        "def create_youtube_comment_rnn(vocabulary, n_outputs):\n",
        "    vocab_size = len(vocabulary) + 1\n",
        "    embedding_dim = 128\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
        "        SpatialDropout1D(0.3),\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),\n",
        "        GRU(64, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(n_outputs, activation='sigmoid' if n_outputs == 1 else 'softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss=\"binary_crossentropy\" if n_outputs == 1 else \"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model, {\"epochs\": 12, \"batch_size\": 32}\n",
        "\n",
        "def create_youtube_comment_cnn(vocabulary, n_outputs):\n",
        "    inputs = Input(shape=(None,))\n",
        "    x = Embedding(input_dim=len(vocabulary) + 1, output_dim=64)(inputs)\n",
        "    x = Conv1D(64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    outputs = Dense(n_outputs, activation=\"sigmoid\" if n_outputs == 1 else \"softmax\")(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\" if n_outputs == 1 else \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model, {\"epochs\": 10, \"batch_size\": 32}"
      ],
      "metadata": {
        "id": "oOOqqgHGvQh5"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}